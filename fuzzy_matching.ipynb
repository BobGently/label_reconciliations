{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Reconciliations with Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from dateutil import parser\n",
    "import re\n",
    "import itertools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code for reconciliations of free text transcript fields.  Have defined a number of fields and a few different methods for each field type. First code for reading in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_clean_data(file_name):\n",
    "    data = {}\n",
    "    with open(file_name, 'r') as in_file:\n",
    "        reader = csv.DictReader(in_file)\n",
    "        headers = reader.fieldnames\n",
    "        for row in reader:\n",
    "            key = row['filename']\n",
    "            #print(key)\n",
    "            xscripts = data.get(key, [])\n",
    "            xscripts.append(row)\n",
    "            data[key] = xscripts\n",
    "            #if reader.line_num >10: \n",
    "            #    break\n",
    "        print(reader.line_num)\n",
    "    return data, headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "['',\n",
      " '',\n",
      " 'subject_id',\n",
      " 'filename',\n",
      " 'user_name',\n",
      " 'created_at',\n",
      " 'Collected by',\n",
      " 'Collection date',\n",
      " 'Collector Number',\n",
      " 'Country',\n",
      " 'County',\n",
      " 'Habitat',\n",
      " 'Description',\n",
      " 'Species',\n",
      " 'State',\n",
      " 'Skip',\n",
      " '',\n",
      " '']\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "data,headers = read_clean_data('Brit_40transcriptsforhabitat.csv')\n",
    "pprint(headers)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Scientific Name:**  \n",
    "    - _Method_: Fuzzy match on tokens in order  (fuzz_ratio) -- Will take a subset as a match if one entirely exists in the other \n",
    "    \n",
    "    \n",
    "    - a. if 100 match on tokens for all 3 - full agree --> 2. \n",
    "    - b. if 100 match on 2 of 3 - maj rule --> 2.\n",
    "    - c. If <100 match on tokens, report two highest scoring and flag --> Done.\n",
    "\n",
    "- 2.To deal with varieties and hybrids (x in plant data) report the longest (string or token umber?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def fuzz_ratio_sn():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Collected By:**  \n",
    "    - _Method_: fuzz.token_sort_ratio() will deal with out of order names and allows for order difference. Fuzzy wuzzy deals will flag punctuation differences (e.g.  A.I  and AI are the same token), just NEED TO CHECK that it deals with capitalization.\n",
    "    \n",
    "    \n",
    "    - a. if 100 match on tokens for all 3 - full agree - show original form.\n",
    "    - b. if 100 match on 2 of 3 - maj. rule - show original form\n",
    "    - c. if less than 100 match on token, [report two highest scoring in original form] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def collected_by():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Habitat and Description:** \n",
    "\n",
    "    -**General Idea**:  _Want the label exactly as written._ \n",
    "        Instructions to transcribers were to write the label exactly as is. Therefore the first step is to look for exact matches. \n",
    "        If there is not an exact match want to maximize information while reducing interpretations. Generally if people do not write what is on the label that is because they have expanded an abbreviation (e.g. hwy --> highway) therefore we want the shortest string length. However, we want the labels with the longest token length to ensure that all words are accounted for.\n",
    "        Main issues with these fields is that the inforamtion often can be found in both fields. For example habitat information writted in the description column and vice versa. Therefore we will first find the best label within each category and then compare across categories.\n",
    "    \n",
    "    -**Within Category Method**: _fuzz.token\\_set\\_ratio_  THEN _fuzz.token_sort_ratio_ -- first looking to see if all the words in one are present in another - would give 100 match. Second look to see the score of the sort ratio - checks the order.\n",
    "    \n",
    "       - a. if all 100s on the set then check on the sort and if 100 choose that comparison. --> go to 2.\n",
    "       - b. if not all 100s find  -- NEED MORE LOGIC HERE>\n",
    "\n",
    "       2. do an exact match.  --> Use the one with the shortest string length and longest word length if there is a difference.  \n",
    "       \n",
    "       \n",
    "\n",
    "    -**Across Category Method**:\n",
    "    - THEN just taking frequency based approach for both habitat and location and secondarily - ask if location and habitat match too much, significant overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "[(100, 0, 1), (100, 0, 2), (100, 1, 2)]\n"
     ]
    }
   ],
   "source": [
    "#pprint(data['BRIT118113'])\n",
    "test = data['BRIT118160']\n",
    "#pprint(test)\n",
    "#lst=[]\n",
    "l = len(test)\n",
    "pprint(l)\n",
    "n=len(test)\n",
    "pprint(n)\n",
    "lst=[]\n",
    "for x in range(n):\n",
    "    for y in range(x+1,n):\n",
    "        score = fuzz.token_set_ratio(test[x]['Habitat'],test[y]['Habitat'])\n",
    "        ### for all 100 take the one with the more words\n",
    "        # if there are different lengths report the flag , most likely there was a shorter transcript in this set. \n",
    "        # \n",
    "        #score2 = fuzz.token_sort_ratio(test[x]['Habitat'],test[y]['Habitat'])\n",
    "        #print(score,x,y)\n",
    "        lst.append((score,x,y))\n",
    "        \n",
    "slst=sorted(lst,key=lambda x: x[0], reverse=True)\n",
    "\n",
    "pprint(slst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reconcile_text_fields(file_name,field):\n",
    "    #test = file_name[field]\n",
    "    #need to turn it to do it on all the subjects\n",
    "    for key, xscripts in data.items():\n",
    "        l = len(xscripts)\n",
    "        #for x in range(l-1):\n",
    "            #if not xscripts[l][field]:\n",
    "            #    print(xscripts[l][field])\n",
    "            #    flag='all blank'\n",
    "            #    print(xscripts[field])\n",
    "        #counts = Counter([xscripts[field] for x in xscripts if x[field] and x[field].lower()])\n",
    "        #if not len(counts):\n",
    "        #    flags[key] = dict(flag=FLAGS[4], value='', top_count=0, blank_count=len(xscripts))\n",
    "        l = len(xscripts)\n",
    "        for x in range(n):\n",
    "            for y in range(x+1,n):\n",
    "                score = fuzz.token_set_ratio(xscripts[x][field],xscripts[y][field])\n",
    "                print(key,score,x,y)\n",
    "        break\n",
    "    #    n = len(test)\n",
    "    #    pprint(test)\n",
    "    return(key)\n",
    "    #return (scores,comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRIT118134 100 0 1\n",
      "BRIT118134 100 0 2\n",
      "BRIT118134 100 1 2\n"
     ]
    }
   ],
   "source": [
    "test = reconcile_text_fields(data,'Habitat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
